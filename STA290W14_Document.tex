\documentclass[11pt]{report}

\usepackage{amsmath,amsthm,amssymb,epsf,eucal}
\usepackage{bm} %% Bold Math
\usepackage{graphicx,psfrag}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{color}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{hyperref}

\newtheorem{claim}{Claim}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newcommand{\R}{\mathbb{R}}
\newcommand{\bs}{\boldsymbol}
\newcommand{\newchapter}[2]{
	\chapter{#1}
	\addtocontents{toc}{\vspace{.1in} \hspace{.25in} $\cdot$ #2 \par}
}

\DeclareMathOperator*{\argmin}{\arg\!\min}

\newcommand*{\titleTH}{\begingroup % Create the command for including the title page in the document
	\center
	\vspace*{\baselineskip} % Whitespace at the top of the page
	\vspace{2.5in}
	{\Huge\bfseries STA290}\\[\baselineskip] % First part of the title, if it is unimportant consider making the font size smaller to accentuate the main title
	{\Huge\texttt{Winter 2014}}\\[\baselineskip] % Main title which draws the focus of the reader
	{\Large \textit{Selected presentation materials}}\par % Tagline or further description
	\vspace*{3\baselineskip} % Whitespace at the bottom of the page
\endgroup}

%\setlength{\parindent}{20pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\titleTH %Title page command
\thispagestyle{empty}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

 %BEGIN CONSTRUCTION OF CHAPTERS



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newchapter{Coordinate free orthogonal projections for fixed vectors}{2/13/14}
\chapter{Abstract vector spaces}

\section{Coordinate free projections}


In this chapter we consider some abstract vector space, call it $V$, which has a norm $\|\cdot \|$ induced by an inner product $\langle \cdot, \cdot \rangle$ (i.e. $\|v\|^2 := \langle v, v\rangle\rangle$). The elements of $V$ are called vectors but since we are considering abstract spaces we do not necessarily have access to the ``coordinates'' of each vector. What we do have is the ability to construct an orthonormal basis of $V$, usually denoted something like $\phi_1,\phi_2, \ldots, \phi_d$, where $d$ is the dimension of $V$. Then each vector in $v$ has a decomposition in terms of these basis vectors. This note explores to computing things like norms and inner products and taking projections with these basis vectors. 



The idea is that $V$ will denote something like points in space. In physics, points in space should exist independently of what coordinate system one uses. In particular, the notion of distance and inner product exist {\emph before} we attach a coordinate system to this space. The following chapter talks about how to work with such vectors. The main story is that one can choose a particular orthonormal basis and then the coefficients of the basis decomposition behave exactly as regular coordinates in $\mathbb{ R}^d$. Moreover, if one wants to project a vector in $V$ to a linear subspace $M\subset V$ then by choosing the basis vectors appropriately one can easily perform the desired projection by simply truncating the basis representation.


\begin{claim}[{\bf Coefficients are coordinates}] Let $\phi_1,\ldots, \phi_d$ be an orthonormal basis for $V$. Then any $v\in V$ has a unique representation 
\[\boxed{ v = \sum_{k=1}^d c_k \phi_k,\,\,\text{where $c_k = \langle v, \phi_k\rangle$}.} \]
 Moreover, these basis coefficients behave exactly like coordinates in regular Euclidean space so that if $v = \sum_{k=1}^n c_k \phi_k$ and $w = \sum_{k=1}^n d_k \phi_k$, then
\[
\boxed{\langle v,w\rangle = \sum_{k=1}^d c_kd_k.}
\]
In particular,  $\| v \|^2 = \sum_{k=1}^d c_k^2$.
\end{claim}



Now suppose  $M\subset V$ is an $m$-dimensional linear subspace (the zero vector should be in here). We can define $M^{\perp}$ to be the set of all vectors in $V$ which are orthogonal to  every vector in $M$. In this case we can write $M \oplus M^{\perp} = V$ to signify that every $v\in V$ has a unique decomposition $v_M + v_M^\perp$ where $v_M\in M$ and $v_M^\perp \in M^\perp$. Often, in statistics, one needs to project a vector $v\in V$ to a subspace $M$. This operation is denoted $P_M v$ and is technically defined as $P_Mv := \argmin_{w \in M} ||w-v||^2 $. The following theorem shows that with a judicious choice of your orthonormal basis this projection is easily calculated

\begin{claim}[{\bf Projections are easy}]\label{proj1} If one constructs the orthonormal basis $\phi_1,\ldots, \phi_d$ of $V$ in such a way that
\begin{align*}
	M &= span\{\phi_1,...,\phi_m\} \label{span} 
\end{align*}
then for any vector $v=\sum_{k=1}^d c_k \phi_k$ the projection to $M$ is computed by truncating the decomposition to m:
\begin{align}
\boxed{P_Mv = \sum\limits_{k=1}^m c_k\phi_k }.
\end{align}
Moreover, since $P_Mv$ must be orthogonal to  $P_{M^{\perp}}v$, and $P_{M^{\perp}}v = v - P_Mv$, we have that 
\[\boxed{P_Mv \perp (v - P_Mv).}\]

\end{claim}



The following is a simple consequence of the above claim, but it will be important later so we state it here
\begin{corollary}\label{cor1}
If $M$ is a linear subspace of $V$ and $v,w \in V$ then
\[\langle v, P_M w\rangle = \langle P_M v, P_M w\rangle = \langle P_M v,  w\rangle. \]  
\end{corollary}




% For any two fixed vectors $z_1,z_2$ that are of equal dimension,
% \begin{align*}
% 	\langle z_1,z_2\rangle = z_1\cdot z_2 = z_1^{T}z_2
% \end{align*}

% Suppose $M$ is an $m$-dimensional linear space that is a subset of $\R^d$. Consequently, define $M^{\perp}$ as the orthogonal compliment of $M$ that has dimension $d-m$ which can be defined with $\phi_k$ as mutually orthogonal vectors for $k = 1,...,d$ such that
% \begin{align*}
% 	M &= span\{\phi_1,...,\phi_m\} \label{span} \\
% 	M^{\perp} & = span\{\phi_{m+1},...,\phi_d\} 
% \end{align*}

% Any vector $z \in \R^d$ can be uniquely represented as $x_1 + x_2 = z $ where $x_1 \in M$ and $x_2 \in M^{\perp}$, which is to say


% \begin{align*}
% 	M \oplus M^{\perp} = \R^d
% \end{align*}

% \vspace{.1in}


% For any $y \in \R^d$ notice that 
% \begin{align*}
% 	y &= \sum\limits_{k=1}^d \langle y, \phi_k\rangle\phi_k \\
% 	||y||^2 &= \langle y,y\rangle = \sum\limits_{k=1}^d \langle y, \phi_k\rangle^2 
% \end{align*}

% \vspace{.1in}

% and the projection of $y$ onto $M$ is defined as 
% $P_My = \argmin_{w \in M} ||w-y||^2 $, which can be expressed as

% \begin{align}
% \boxed{P_My = \sum\limits_{k=1}^m \langle y, \phi_k\rangle\phi_k }
% \end{align}

% The following relation holds for the space $M$ and its orthogonal compliment $M^{\perp}$ \vspace{.1in}
% \begin{align*}
% \boxed{P_My \perp P_{M^{\perp}}y \qquad i.e. \qquad P_My \perp (y - P_My) \bigg.}
% \end{align*}

%%%%%%%%%%%%%
%END CHAPTER%
%%%%%%%%%%%%%





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Projections for Gaussians}


If we are working in an abstract vector space $V$, what do we even mean by a Gaussian vector in $V$? A random vector $Y$ is said to be Gaussian if $\langle v,Y\rangle$ is a Gaussian random variable for all $v\in V$. Now we want some notion of $Y\sim \mathcal N(0,\sigma^2 I_d)$. To be able to say $Y\sim \mathcal N(0,\sigma^2 I_d)$ we simply require that there exists a orthonormal basis representation $Y= \sum_{k=1}^d d_k \psi_k$ where $d_1,\ldots, d_d$ are iid $\mathcal N(0,\sigma^2)$. 

A fundamental fact about independent mean zero Gaussian random variables is that they are invariant under rotations. In particular if $W \sim \mathcal N\left(0, \sigma^2 I_d\right)$ where $W$ is a random vector in Euclidean space then $UW \sim \mathcal N\left(0, \sigma^2I_d\right)$ for any orthogonal rotation matrix ($U$ is a rotation if  $I = U^{t}U$). In fact, independent mean zero Gaussians make up the only multivariate distribution with independent coordinates that is rotationally invariant. 
You can think of left multiplication by a rotation matrix as simply changing basis. Therefore if  $Y$ is an abstract random vector that satisfies $Y\sim \mathcal N(0,\sigma^2 I)$, then {\em any} basis decomposition of $Y$ should give iid $\mathcal N(0,\sigma^2)$ coefficients. 

\begin{claim}
Suppose $Y$ is an abstract Gaussian random vector in $V$ which satisfies $Y\sim \mathcal N(0,\sigma^2 I)$. If $\phi_1,\ldots, \phi_d$ is a orthonormal basis of $V$ then 
 
 \[ 
 \boxed{Y = \sum_{k=1}^d c_k \phi_k \text{ implies } c_1,\ldots, c_d \text{ are iid $\mathcal N(0,\sigma^2)$.}}
 \]
\end{claim}

Once we have the above theorem the following corollary is easy to prove.

\begin{corollary}\label{cor2}
Suppose $\phi_1,\ldots, \phi_d$ is a orthonormal basis of $V$ and $Y\sim \mathcal N(0,\sigma^2 I_d)$. Suppose $M$ is an $m$-dimensional linear subspace of $V$ which is spanned by $\phi_1,\ldots, \phi_m$. If $Y = \sum_{k=1}^d c_k \phi_k$ and $v = \sum_{k=1}^d v_k \phi_k$ then the following three statements are true:
\begin{enumerate}[(i)]
\item $||P_MY||^2 = c_1^2+\cdots c_m^2 \sim \sigma^2 \chi^2_m$;
\item $P_MY$ is independent of  $Y-P_MY$;
\item $var(\langle v,Y\rangle) = var( v_1 c_1+\cdots v_dc_d) =  \sigma^2 \| v \|^2$.
\end{enumerate}
\end{corollary}

Just as a simple example of power and simplicity of these statements notice that one can easily recover the fact that $\overline Y$ and $S^2$ are independent.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Application to linear models}

The basic coordinate-free linear model 
\[
Y = \mu + Z
\]
where $Y, \mu$ and $Z$ all take values in a vector space $V$, the mean vector $\mu$ is assumed to be in a linear subspace $M\subset V$ and $Z\sim \mathcal N(0,\sigma^2I)$. 

%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gauss-Markov}
The Gauss-Markov theorem effectively says that the optimal estimate of $\mu$ is the projection of $Y$ onto $M$, i.e. $\hat\mu = P_M Y$. 
\begin{claim}[Gauss-Markov]
For any $v\in M$, the minimum variance linear unbiased estimate of $\langle v,\mu\rangle$ is given by $\langle v,\hat\mu\rangle$ where $\hat\mu := P_M Y$. 
\end{claim}


\begin{proof}
There are two main facts in the proof of the Gauss-Markov Theorem. First, notice 
\begin{align}
\langle v,\hat\mu\rangle &= \langle v,P_M Y \rangle =\langle P_Mv,P_M Y \rangle = \langle P_Mv,Y \rangle \label{GMform}
\end{align}
which follows by Corollary \ref{cor1}. 
The second fact is to simply unravel what it means for a linear estimate $\langle w,Y\rangle$  to be unbiased. Notice that $E\langle w,Y\rangle = \langle w, \mu\rangle= \langle P_M w, \mu\rangle$ so the unbiased constraint becomes
\begin{equation}
\langle P_Mw,\, \mu\rangle =  \langle P_M v,\, \mu\rangle\text{ for all $\mu\in M$}.
\end{equation} 
Now this implies that $P_Mw = P_Mv$ (because the above formula implies $\langle  P_Mw - P_Mv, \nu \rangle = 0$ for  $\nu := P_Mw - P_Mv\in M$). To summarize
\begin{quote}
\em
A necessary and sufficient condition that $\langle w, Y\rangle$ be a linear unbiased estimate of $\langle v, \mu\rangle$ is that $P_M w = P_M v$.
\end{quote}
By (\ref{GMform}) we know that the Gauss-Markov estimate is a linear unbiased estimate.
Now it is easy to show the Gauss-Markov estimator has minimum variance among linear unbiased estimates
\begin{align*}
\text{var}(\langle w, Y\rangle) &= \text{var}(\langle w, Z\rangle) \\
 &= \sigma^2\|w \|^2, \quad\text{by \ref{cor2}} \\
 &= \sigma^2\|P_Mw \|^2 + \sigma^2\|P_M^\perp w \|^2\\
 &\geq \sigma^2\|P_Mw \|^2 \\
 & = \sigma^2\|P_M v \|^2,\quad\text{if $\langle w, Y\rangle$ is unbiased} \\
 & = \text{var}(\langle P_M v, Y\rangle) \\
 & = \text{var}(\langle v, \hat\mu\rangle).
\end{align*}

\end{proof}


\subsection{Regression}
Notice that the Gauss-Markov theorem has implications for ANOVA, regression and most other linear statistical models. In this section we explore the Gauss-Markov theorem in the context of regression. The standard regression setup is 
\[
Y = X \beta + Z
\]
where $X$ is the design matrix with $p$ columns  and $\beta$ is a $p$ dimensional coefficient vector of unknowns and $Z\sim \mathcal N(0,\sigma^2 I)$. This is indeed equivalent to our basic linear model
\[Y = \mu + Z \]
where now $\mu$ is assumed to be in the linear space $M:= \{ \beta_1 x_1+\cdots \beta_p x_p:\beta_k\in \Bbb R\}$ where $x_k$ is the $k^\text{th}$ column of $X$. 

To reconcile the OLS estimate  $\hat\beta = (X^tX)^{-1}X^t Y$ and the Gauss-Markov estimate $\hat\mu = P_M Y$ simply notice that the OLS estimate of $\mu$ is simply $X\hat \beta = X(X^tX)^{-1}X^t Y$ which is the same as $P_M Y$. In particular, $X(X^tX)^{-1}X^t$ is the coordinate-matrix form of $P_M$. This is easy to see when one adds the additional assumption that the columns of $X$ are orthonormal. In this case we know $X^tX = I$ and from Claim \ref{proj1} we know how to do the projection:
\begin{align*}
P_M Y &= \sum_{k=1}^p \langle Y, x_k\rangle x_k \\
&= XX^tY,\quad\text{since $X^tY$ gives $\langle Y, x_k\rangle$}\\
&= X(X^tX)^{-1}X^tY \\
&= X\hat \beta.  
\end{align*}

Now a few nice facts from regression automatically follow from the basic facts of projection. In particular we know that $P_M$ is independent of $Y-P_MY$ from Corollary \ref{cor2}. This translates to the fact that 
\begin{quote}
\em The estimated residuals $Y - X\hat \beta$ and the estimated mean $X\hat \beta$ are independent (and orthogonal as vectors).
\end{quote}
We also know that when $\mu$ is the zero vector (i.e. there is no signal) then $\|\hat \mu \|^2 = \|X\hat\beta \|^2 = \|P_M Y \|^2\sim  \sigma^2 \chi^2_p$. In a similar manner it is also clear that $\|Y - \hat \mu \|^2\sim \sigma^2 \chi^2_{n-p}$. This gives the classic $F$ test for $H_0:\mu=0$
\begin{quote}
\em
Under the null hypothesis that $\mu = 0$ the random variable $F:= {\|\hat \mu \|^2}/{\|Y - \hat \mu \|^2}$ has the same distribution as ${Z_1}/{Z_2}$ where $Z_1\sim \chi^2_p$ and $Z_2\sim \chi^2_{n-p}$ are independent random variables.
\end{quote}
Note that the independence again comes from the fact that projections of spherical Gaussians are independent of their residuals. This way of viewing the $F$ test is very natural. ${\|\hat \mu \|^2}/{\|Y - \hat \mu \|^2}$ measures the size of $\| \hat \mu\|^2$ normalized by and estimate of $\sigma^2$ so $F$ is unit free. One can also understand $\| \hat \mu\|^2$ as quantifying the explained variability in $Y$ and $\|Y - \hat \mu \|^2$ as the unexplained variability in $Y$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Likelihood ratio statistic}

The basic goal of this section is to use our understanding of Gaussian projections to understand why $2\times(\text{log ratio statistic})= 2\Delta \ell(\hat \mu)$ is distributed $\chi^2_{\Delta \text{dim}}$ under the null hypothesis where $\Delta \text{dim}$ to denotes the different in dimensions going from the null model to the full model.

To start lets examine the classic linear model
\[Y = \mu + Z,\quad Z\sim\mathcal N(0,\sigma^2I). \]
Our hypothesis test is
\begin{align*}
&H_0: \mu\in M_0\\
&H_1: \mu \in M_1
\end{align*}
where $M_0\subset M_1 \subset V$ are all linear vector spaces. Suppose $M_0$, $M_1$ and $V$ have dimensions $d_0, d_1$ and $d$ respectively. We are free to choose our orthonormal basis of $V$ as 
\[\underbrace{\underbrace{\phi_1,\ldots, \phi_{d_0}}_{\text{these span $M_0$}},\,\, \psi_{1},\ldots, \psi_{d_1 - d_0}}_{\text{these span $M_1$}},\,\, \xi_1,\ldots \xi_{d_1 - d}.\]
One can use the random coefficients under this basis to generate a log-likelihood for $\mu$ given the data as follows 
\begin{align*}
\ell(Y|\mu) =-\frac{1}{2\sigma^2} \|Y - \mu \|^2 - \frac{n}{2}\log(2\pi\sigma^{2})
\end{align*}
Now it's clear that the MLE estimates $\hat\mu_{M_0}$ and $ \hat\mu_{M_1}$ just equal the projections $P_{M_0}Y$ and $P_{M_1}Y$ since projections are characterized by the minimizer of the square norm. Therefore
\begin{align*}
2\Delta \ell(Y|\hat\mu) &= 2\ell(Y|P_{M_1}Y) - 2\ell(Y|P_{M_0}Y) \\
 &= -\frac{1}{\sigma^2} \|Y - P_{M_1}Y \|^2 + \frac{1}{\sigma^2} \|Y - P_{M_0}Y \|^2 \\
 &= -\frac{1}{\sigma^2} \left[\sum_{k=1}^{d_1-d}{\langle Y,\xi_k \rangle^2}\right] + \frac{1}{\sigma^2}  \left[ \sum_{k=1}^{d_1-d}{\langle Y,\xi_k \rangle^2}+  \sum_{k=1}^{d_1-d_0}{\langle Y,\psi_k \rangle^2}\right] \\
 &= \frac{1}{\sigma^2}  \sum_{k=1}^{d_1-d_0}{\langle Y,\psi_k \rangle^2} \sim \chi^2_{d_1 - d_0}, \qquad \text{by Corollary \ref{cor2}}
\end{align*}

The interesting thing is that the distribution $\chi^2_{d_1 - d_1}$ persists as an approximation for $2\Delta \ell(\hat\mu|Y)$  under much greater generality. The conditions for this approximation  are, loosely, that the the statistical model has the local asymptotic normality property (LAN for short), the signal-to-noise ratio of the observations goes to infinity, and the two models $M_0$ and $M_1$ are locally linear with local dimension $d_0$ and $d_1$ respectively. If one doesn't have the local linear property of $M_0$, say, then the asymptotic distribution can be a mixture of chi-square random variables.
To see why this is true we start with a small extension of the classic linear model
\[ Y= \mu + {\sigma} Z \]
where now $ M_0\subset M_1$ are  only assumed locally linear  with local dimension $d_0$ and $d_1$ respectively (let suppose the embedded space $V$ is still a linear vector space). We will analyze  $2\Delta \ell(Y|\hat\mu)$  in the case that $\sigma\rightarrow 0$  to model the situation where the signal-to-noise of the observations goes to infinity.  A good example to keep in mind is  the case where  $V=\Bbb R^5$, $M_1=\Bbb R^3$ and $M_0$ is the two dimensional sphere embedded in $M_1$. 

To analyze the asymptotic distribution of $2\Delta \ell(Y|\hat\mu)$ in this case suppose the true $\mu$ is in  $ M_0$.
Notice that we are still free to set down a orthonormal basis of $V$
\[\phi_1,\ldots, \phi_{d_0},\,\, \psi_{1},\ldots, \psi_{d_1 - d_0},\,\, \xi_1,\ldots \xi_{d_1 - d}\]
but we do so in such a way that ${\phi_k}$ spans the local linear approximation of $M_0$ around the true $\mu$, and similarly that ${\phi_k, \psi_j}$ spans the local linear approximation of $M_1$ around the true $\mu$.
Now as $\sigma\rightarrow 0$, the $Y$ gets closer to $\mu$ and the local linear approximation of $M_0$ and $M_1$ becomes accurate. In this case, the estimates
\[ \text{$\hat\mu_{\tiny M_0}\approx P_{L_\mu M_0} Y$ and $\hat\mu_{\tiny M_1}\approx P_{L_\mu M_1} Y$}\]
 where $L_\mu M_0$ and $L_\mu M_1$ denote the local linear approximations at $\mu$. Therefore
\begin{align*}
2\Delta \ell(Y|\hat\mu) 
&= 2\ell(Y|\hat\mu_{M_1}) - 2\ell(Y|\hat\mu_{M_0}) \\
&\approx 2\ell(Y|P_{L_\mu M_1}Y) - 2\ell(Y|P_{L_\mu M_0}Y) \\
 &= \frac{1}{\sigma^2}  \sum_{k=1}^{d_1-d_0}{\langle Y,\psi_k \rangle^2} \sim \chi^2_{d_1 - d_0}.
\end{align*}


It now becomes clear what happens when the local linear approximation doesn't hold. For example suppose $V=\Bbb R^5$, $M_1=\Bbb R^3$ and $M_0=\{ \|v\|_p=1: v\in V\}$ where $p<1$. In this case $M_0$ has a kink at the each axis. Suppose the true $\mu$ is at the tip of one of those kinks. Then as $\sigma^2\rightarrow 0$, the $Y$ gets closer to $\mu$ and the resulting estimate $\hat\mu_{M_0}$ starts to behave like the projection of $Y$ onto a half line. This makes the term $\frac{1}{\sigma^2} \|Y - P_{M_0}Y \|^2$ in $2\Delta \ell(Y|\hat\mu) $ behave like a mixture of chi-squares depending on if the projection of $Y$ lands on the tip of the half line or to the interior of the half line. 

Finally, what happens when one doesn't have the nice classical linear model $Y = \mu + \sigma Z$? In this case we observe data, in the form of a vector of observations $Y$, and we have access only to a log-likelihood surface $\ell(Y|\mu)$. The local asymptotic normality condition essentially says that the MLE $\hat\mu$ (computed under model $M_1$) has the distribution $\hat\mu \sim \mathcal N(\mu, F^{-1}/n)$ where $F$ is something like the expected Hessian $\ell$ with one data point. The idea is that, in this case, the likelihood surface of the whole data set $\ell(Y|\mu)$ is well approximated by the likelihood surface of $\hat\mu$ under the model $\hat\mu \sim \mathcal N(\mu, F^{-1}/n)$. This suggested that we can make a new ``data vector'' $Y_{new} = F^{1/2}\hat\mu$ where $Y_{new}= \theta + Z$ and $Z\sim \mathcal N(0, n^{-1}I)$. This is the exact situation we were in earlier, with the signal to noise going to infinity, and the chi-square results still hold.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Gaussian $E(X_0|X_1,\ldots, X_n)$ as projection}


Most of what we have done with our abstract vector space results could have been don't just as easy (albeit a bit more messy) with coordinates. This hides the true power and generality of the abstract projection results. In this section we analyze projection of random variables which have no natural coordinate vector representation. In particular we show that Gaussian conditional expectation is simply linear projection onto the linear span of the observations. 

Suppose $X_0,X_1,\ldots, X_n$ are jointly Gaussian random variables. We can also suppopse, without loss of generality, that $E(X_k)=0$ for all $k=0,1,\ldots, n$.
To derive the conditional distribution $X_0|X_1,\ldots, X_n$ notice first
\[
{E(X_0|X_1,\ldots, X_n) = a_1 X_1 + \cdots a_n X_n,\text{ for some constants $a_k$.}}
\]
In particular if we let $M$ denote the linear vector space $\{a_1 X_1 + \cdots a_n X_n: a_k\in\Bbb R\}$ then $E(X_0|X_1,\ldots, X_n)\in M$. The second fact is that  $E(X_0|X_1,\ldots, X_n)$ minimizes the mean square distance to $X_0$. 
\begin{equation}\label{condexp1}
{E(X_0|X_1,\ldots, X_n) = \text{arg}\min_{X \in M} E(X_0 - X)^2. }
\end{equation}
 This establishes that $E(X_0|X_1,\ldots, X_n)$ is a projection. To be specific, let $V=\{a_0 X_0 + \cdots a_n X_n: a_k\in\Bbb R\}$ be the full set of linear combinations of $X_0,X_1,\ldots, X_n$. This is obviously a linear space of random variables. To make $V$ an abstract vector space we need an inner product for elements of $V$. The natural inner product is given by covariance:
\[ \text{For any $X,Y\in V$ define $\langle X,Y\rangle :=\text{cov}(X,Y)$.}  \]
Notice that the norm $\| X\|$ gives the standard deviation of $X$. Using this inner product structure it is clear that equation (\ref{condexp1}) gives
\begin{equation}
\label{condexp2} 
\boxed{E(X_0|X_1,\ldots, X_n) = P_M X_0.} 
\end{equation}
We will use our understanding of projection to derive $E(X_0|X_1,\ldots, X_n)$ but first we mention that,
as a consequence of this projection, we have that 
\begin{equation} 
\label{indep}
\boxed{X_0 - P_M X_0 \perp X_1,\ldots, X_n}
\end{equation}
since $X_k\in M$
where $\perp$ in this case means uncorrelated (i.e. independent since we are working with Gaussians). The above fact has an important consequence which will allow us to derive $\text{var}(X_0|X_1,\ldots, X_n)$
\[
\boxed{\text{var}(X_0 - P_M X_0 ) \overset{(\ref{indep})}= \text{var}(X_0 - P_M X_0 |X_1,\ldots, X_n) = \text{var}(X_0  |X_1,\ldots, X_n)}
\]
where the last equality follows since $P_M X_0= E(X_0|X_1,\ldots, X_n)$ is not random given $X_1,\ldots, X_n$.
So $\text{var}(X_0  |X_1,\ldots, X_n)$ is simply the marginal variance of the residual $X_0 - P_M X_0$.  If one has a fast algorithm for computing  $E(X_0|X_1,\ldots, X_n)$  and for simulating $X_0,\ldots, X_n$ then one can easily generate a conditional simulation of $X_0  |X_1,\ldots, X_n$. In particular since $X_0  |X_1,\ldots, X_n\sim \mathcal N(E(X_0|X_1,\ldots, X_n), \text{var}(X_0|X_1,\ldots, X_n))$ to generate a simulation  one simply adds a random $Z$ to  $E(X_0|X_1,\ldots, X_n) $ where $Z\sim \mathcal N(0, \text{var}(X_0|X_1,\ldots, X_n))$. By the above equation we can generate such a $Z$ by simply generating the prediction residual $X^*_0 - P_{M^*} X_0^*$ on a new independent simulation $X_0^*, \ldots, X_n^*$.

Now lets use our understanding of projections to compute (\ref{condexp2}). In paricular we need to find and orthonormal basis $\phi_1,\ldots, \phi_n$ of $M$ and then write $P_M X_0 = \sum_{k=1}^n \langle X_0, \phi_k \rangle \phi_k$. Notice that orthonormality in this case means uncorrelated. To uncorrelate $X_1,\ldots, X_n$ simply multiply by $\Sigma_{nn}^{-1/2}$
\[\begin{pmatrix}
\phi_1 \\ \vdots \\\phi_n
\end{pmatrix}
=
\Sigma_{nn}^{-1/2} \begin{pmatrix}
X_1 \\ \vdots \\X_n
\end{pmatrix}.
  \]
This gives our orthonormal basis of $M$, the coefficient of which  can be computed as
 \begin{align*}
  \begin{pmatrix}
\langle X_0,\phi_1\rangle \\ \vdots \\\langle X_0,\phi_n\rangle
\end{pmatrix}
&=E\left[\begin{pmatrix}
\phi_1 \\ \vdots \\\phi_n
\end{pmatrix} X_0\right]  = E\left[\Sigma_{nn}^{-1/2} 
 \begin{pmatrix} X_1 \\ \vdots \\X_n \end{pmatrix}
 X_0\right] = \Sigma_{nn}^{-1/2} \Sigma_{n0}  
\end{align*}
Now 
\begin{align*}
E(X_0|X_1,\ldots, X_n)&=P_M X_0= \sum_{k=1}^n \langle X_0, \phi_k \rangle \phi_k \\
 & = \begin{pmatrix}
\langle X_0,\phi_1\rangle \\ \vdots \\\langle X_0,\phi_n\rangle
\end{pmatrix}^t
\begin{pmatrix}
\phi_1 \\ \vdots \\\phi_n
\end{pmatrix}\\
& =\Sigma_{0n}\Sigma_{nn}^{-1/2}\begin{pmatrix}
\phi_1 \\ \vdots \\\phi_n
\end{pmatrix}\\
& =\Sigma_{0n}\Sigma_{nn}^{-1} \begin{pmatrix}
X_1 \\ \vdots \\X_n
\end{pmatrix}.
\end{align*}
Also,
\begin{align*}
\text{var}(X_0|X_1,\ldots, X_n)&= \text{var}(X_0- P_MX_0) \\
& = \text{var}(X_0) - 2\text{cov}(X_0, P_MX_0) +  \text{var}(P_MX_0) \\
& = \Sigma_{00} - 2 \Sigma_{0n}\Sigma_{nn}^{-1}\Sigma_{n0} +\Sigma_{0n}\Sigma_{nn}^{-1}\Sigma_{nn}\Sigma_{nn}^{-1}\Sigma_{n0}\\
& = \Sigma_{00} -  \Sigma_{0n}\Sigma_{nn}^{-1}\Sigma_{n0}
\end{align*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Fubini}

\chapter{Ideas Involving Bayesian Risk}

In this chapter we will explore some ideas involved in finding a Bayes rule (or Bayes estimator), and its associated Bayes risk under a specific loss function. 


Consider the family of Poisson distributions $\mathcal{P}(\lambda)$, $\lambda>0$, with p.m.f. given by 
$$p(x|\lambda) = e^{-\lambda}\frac{\lambda^x}{x!}, \quad x=0,1,2, \dots, $$
where we are interested in estimating $\lambda$ under the loss function
\begin{equation} \label{eq:loss}
l(\lambda,\delta(X)) = \frac{(\lambda-\delta(X))^2}{\lambda}.
\end{equation}
	
\begin{claim}
The Gamma family of distributions forms a conjugate family of priors for $\lambda$.
		
Specifically, let $\lambda \sim Gamma(\alpha,\beta)$ be a prior on $\lambda$ with prior hyper parameters $\theta_0 = (\alpha,\beta)$. Then the posterior is $\lambda|x \sim Gamma\left(\alpha+x, \frac{\beta}{\beta+1}\right)$, with posterior hyper parameters $\theta_1 = \left(\alpha+x, \frac{\beta}{\beta+1}\right)$. 
\end{claim}
		
Under this Gamma prior, a natural question is to find the associated Bayes estimator, or Bayes rule, $\widehat{\lambda}_\pi$.

By definition, a Bayes estimator $\delta^*(X)$ is a decision rule such that it minimizes the posterior risk; i.e.  
$$r(\delta^*(x)|x) = \inf_{\delta}\; r_\pi(\delta|x) $$
where $r_\pi(\delta|x)$ is defined as 
\begin{equation}\label{eq:postrisk}
r_\pi(\delta|x) = \mathbb{E}_{\lambda|x}(l(\lambda,\delta(X))|X=x) = \int_\Lambda l(\lambda,\delta(X)) \pi(\lambda|x) \; d\lambda,
\end{equation}
and the expectation is taken with respect to the posterior distribution of $\lambda$ given $X=x$, and $\lambda$ is distributed as $\pi$. 

In order to find this desired rule, $\widehat{\lambda}_\pi = \delta^*(X)$, and subsequently its associated Bayes risk, there are a number of ways to go about performing the actual computations. Our goal in this note is to show that not all of these methods are ``created equal'' in terms of computational time, which is especially relevant when taking an in-class exam. \newline

 
\underline{\textbf{Method I:}} Finding $\widehat{\lambda}_\pi$ directly. \newline
Using equation (\ref{eq:postrisk}), we can substitute in our loss function (\ref{eq:loss}), and arrive at  
\begin{align*}
r_\pi(\delta|x) 
& = \mathbb{E}_{\lambda|x}\left(\frac{(\lambda-\delta)^2}{\lambda}\right )  = \mathbb{E}_{\lambda|x}\left(\lambda\right) - 2\delta + \delta^2\mathbb{E}_{\lambda|x}\left(\frac{1}{\lambda}\right ) 
\end{align*}
We can find the infimum of this Bayes risk over all decision rules $\delta$ by taking the derivative with respect to $\delta$, and setting equal to 0.
		
$$\frac{\partial r_\pi(\delta|x)}{\partial \delta} =  - 2\delta + \delta^2\mathbb{E}_{\lambda|x}\left(\frac{1}{\lambda} \right )$$
$$\implies \quad \delta^*(X) = \frac{1}{\mathbb{E}_{\lambda|x}\left(\frac{1}{\lambda} \right )}$$
This expected value, taken under the posterior distribution, is equal to $\frac{\beta+1}{(\alpha +x-1)\beta}$
\begin{align*}
\mathbb{E}_{\lambda|x}\left(\frac{1}{\lambda} \right) 
& = \int \frac{1}{\lambda} \pi(\lambda|x) \; d\lambda 
= \int \frac{1}{\lambda} \frac{1}{\Gamma(\alpha+x)\left(\frac{\beta}{\beta+1}\right)^{\alpha+x}}\lambda^{\alpha+x-1}e^{-(\frac{\beta+1}{\beta})\lambda} \; d\lambda \\
& = \frac{\Gamma(\alpha+x-1)\left(\frac{\beta}{\beta+1}\right)^{\alpha+x-1}}{\Gamma(\alpha+x)\left(\frac{\beta}{\beta+1}\right)^{\alpha+x}}\int\frac{\lambda^{(\alpha+x-1)-1}e^{-(\frac{\beta+1}{\beta})\lambda}}{\Gamma(\alpha+x-1)\left(\frac{\beta}{\beta+1}\right)^{\alpha+x-1}} \; d\lambda \\
& = \frac{1}{(\alpha+x-1)\left(\frac{\beta}{\beta+1}\right)}
\end{align*}		
Thus, 
$$\delta^*(X) = \widehat{\lambda}_\pi =\frac{(\alpha +x-1)\beta}{\beta+1} $$

\underline{\textbf{Method II:}} A second method for finding this Bayes rule utilizes the following well-known theorem, and a subsequent corollary. 

\begin{theorem}
Given a squared-error loss function $l(\lambda, \delta(X)) = (\lambda - \delta(X))^2$, and a prior $\pi$ on $\lambda$, the Bayes rule is found by taking the expected value of the associated posterior distribution. i.e. $\widehat{\lambda}_\pi = E_{\lambda|x}(\lambda)$. 
\end{theorem}

\begin{corollary}
Let $\omega >0$ be a positive `weight'. Then, for the so-called `weighted squared-error` loss function $l(\lambda, \delta(X)) = \frac{(\lambda - \delta(X))^2}{\omega}$. The Bayes rule is found by solving the following integral:
$$\widehat{\lambda}_\pi = \int_\Lambda\frac{(\lambda - \delta(X))^2}{\omega} \pi(\lambda|x) d\lambda$$
where $\pi$ is a prior on $\lambda$, and $\pi(\lambda|X)$ is the density of the associated posterior distribution. 
\end{corollary}			

For our loss function given in (\ref{eq:loss}) we recognize that we have a ``weighted'' square-error loss. Thus, we identify $\omega = \lambda$, and write  
$$\widehat{\lambda}_\pi = \int_\Lambda\frac{(\lambda - \delta(X))^2}{\lambda} \pi(\lambda|x) d\lambda$$
Now, since our posterior distribution is a gamma with parameters $\left(\alpha+x, \frac{\beta}{\beta+1}\right)$, then taking this posterior density and dividing by $\lambda$ gives a density of a gamma distribution with parameters $\left(\alpha+x-1, \frac{\beta}{\beta+1}\right)$. It follows that the Bayes rule is simply the expected value of this newly-formed gamma distribution. i.e.
$$\widehat{\lambda}_\pi = \frac{(\alpha +x-1)\beta}{\beta+1}.$$
		
Next, we are interested in finding the Bayes risk of this Bayes rule. i.e. $r(\pi, \widehat{\lambda}_\pi)$. 
		
The Bayes risk for any decision rule $\delta$ is defined as 
		$$r(\pi, \delta) = \mathbb{E}_\lambda(R(\lambda,\delta)) = \mathbb{E}_\lambda\left[\mathbb{E}_{x|\lambda}\theta\left(l(\lambda, \delta)\right) \right]$$
		or, by explicitly writing out the expectations,
		$$r(\pi, \delta) = \int R(\theta,\delta) \pi(\lambda)\; d\lambda = \int\left[ \int l(\lambda,\delta)p(x|\lambda) \;dx\right] \pi(\lambda)\; d\lambda$$
		

Note that with $\widehat{\lambda}_\pi = (\alpha +x-1)\frac{\beta}{\beta+1}$, we can write	
$$\widehat{\lambda}_\pi = \gamma x + (\alpha-1)\gamma,\quad \text{where} \quad \gamma = \frac{\beta}{\beta+1}.$$


\underline{\textbf{Method I:}} Compute directly \newline
		\begin{align*}
		r\left(\pi, \widehat{\lambda}\right)
		&  = \mathbb{E}_\lambda\left[\mathbb{E}_{x|\lambda}\left(l(\lambda, \widehat{\lambda})\right) \right] 
		   = \mathbb{E}_\lambda\left[\mathbb{E}_{x|\lambda}\left(\frac{(\lambda-\widehat{\lambda})^2}{\lambda}\right) \right] \\
		&  = \mathbb{E}_\lambda\left[\mathbb{E}_{x|\lambda}\left(\lambda - 2\widehat{\lambda} + \frac{1}{\lambda}\widehat{\lambda}^2 \right) \right] 
		   = \mathbb{E}_\lambda\left[\lambda - 2\mathbb{E}_{x|\lambda}\left(\widehat{\lambda}\right) + \frac{1}{\lambda}\mathbb{E}_{x|\lambda}\left(\widehat{\lambda}^2\right)  \right] \\
		& = \cdots
		\end{align*}
This method quickly becomes a computational nightmare. \newline 


\underline{\textbf{Method II:}} Compute using the MSE: $\mathbb{E}(\lambda-\widehat{\lambda})^2 = Var(\widehat{\lambda}) + [E(\widehat{\lambda}) -\lambda]^2$ 

\begin{align*}
r\left(\pi, \widehat{\lambda}\right)
& = \mathbb{E}_\lambda\left[\mathbb{E}_{x|\lambda}\left(l(\lambda, \widehat{\lambda})\right) \right] 
= \mathbb{E}_\lambda\left[\frac{1}{\lambda}\left(Var_{x|\lambda}(\widehat{\lambda}) + [\mathbb{E}_{x|\lambda}(\widehat{\lambda}) -\lambda]^2 \right) \right] \\
& = \mathbb{E}_\lambda\left[\frac{1}{\lambda}\left(\gamma^2\lambda + [\gamma\lambda + (\alpha-1)\gamma-\lambda]^2 \right) \right] \\
& = \mathbb{E}_\lambda\left[\frac{1}{\lambda}\left(\gamma^2\lambda + \lambda^2(\gamma-1)^2 + 2\lambda\gamma(\gamma-1)(\alpha-1) + \gamma^2(\alpha-1)^2 \right) \right] \\
& = \mathbb{E}_\lambda\left[\gamma^2 + \lambda\frac{\gamma^2}{\beta^2} - 2\frac{\gamma^2}{\beta}(\alpha-1) + \frac{1}{\lambda}\gamma^2(\alpha-1)^2  \right] \\ 		
& = \gamma^2 + \alpha\beta\frac{\gamma^2}{\beta^2} - 2\frac{\gamma^2}{\beta}(\alpha-1) + \frac{1}{(\alpha-1)\beta}\gamma^2(\alpha-1)^2 \\   
& = \gamma^2 + \alpha\frac{\gamma^2}{\beta} - \frac{\gamma^2}{\beta}(\alpha-1)   		
= \gamma^2  + \frac{\gamma^2}{\beta}   		
= \gamma^2\left(1  + \frac{1}{\beta}\right)   	
= \frac{\beta}{\beta+1} \quad \qed 
\end{align*}
where we used the following:
$$Var_{x|\lambda}(\widehat{\lambda}) = \gamma^2\lambda, \quad  \mathbb{E}_{x|\lambda}(\widehat{\lambda}) = \gamma\lambda +(\alpha-1)\gamma, \quad \gamma-1 = \frac{-\gamma}{\beta} $$
\underline{\textbf{Method III:}} Compute by swapping expectations. \newline
Switch the order of expectations, which is allowed by applying Bayes Theorem to change from the prior to the posterior, and Fubini's Theorem which allows us to swap the order of integration.   
$$r(\pi, \delta) =  \int\left[ \int l(\lambda,\delta)p(x|\lambda) \;dx\right] \pi(\lambda)\; d\lambda = \int\left[ \int l(\lambda,\delta)\pi(\lambda|x) \;d\lambda\right] p(x)\; dx =  \mathbb{E}_x\left[\mathbb{E}_{\lambda|x}\left(l(\lambda, \widehat{\lambda})\right) \right]$$
Then, 
\begin{align*}
r\left(\pi, \widehat{\lambda}\right)
& = \mathbb{E}_x\left[\mathbb{E}_{\lambda|x}\left(l(\lambda, \widehat{\lambda})\right) \right] 
= \mathbb{E}_x\left[\mathbb{E}_{\lambda|x}\left(\lambda - 2\widehat{\lambda} + \frac{1}{\lambda}\widehat{\lambda}^2 \right) \right] \\
& = \mathbb{E}_x\left[\widehat{\lambda} + \frac{\beta}{\beta+1} - 2\widehat{\lambda} + \frac{1}{\widehat{\lambda}}\widehat{\lambda}^2  \right] 
= \mathbb{E}_x\left[\frac{\beta}{\beta+1}  \right] \\
& = \frac{\beta}{\beta+1}  \qed
\end{align*}
where we used 
$$\mathbb{E}_{\lambda|x}\left(\lambda\right) = \frac{(\alpha +x)\beta}{\beta+1} = \frac{(\alpha +x - 1)\beta}{\beta+1} + \frac{\beta}{\beta+1} = \widehat{\lambda} + \frac{\beta}{\beta+1},\text{ and } \mathbb{E}_{\lambda|x}\left(\frac{1}{\lambda}\right) = \frac{1}{\widehat{\lambda}}. $$

\chapter{Sufficient, Complete, and UMVU Estimators}
This problem comes from the Fall 2009 pre-qual. 

\emph{
Let $\underline{X} = (X_1, \dots, X_n)^T$ from Uniform$(\theta_1-\theta_2, \theta_1+\theta_2)$ where $\theta_1 \in \mathcal{R}$ and $\theta_2 > 0$ are unknown. Consider estimating $\theta_1, \theta_2$. }
\newline
(a) Show $T(\underline{X}) = (X_{(1)}, X_{(n)})$ is sufficient and complete.

\textbf{Sufficiency:} Using the Fisher-Neyman Factorization,
\begin{align*}
f(\underline{X}) &= \prod_{i=1}^n f(x_i) = 
 = \left( \frac{1}{(\theta_1+\theta_2) - (\theta_1-\theta_2)} \right)^n
\cdot \prod \mathbf{1}(x_i > \theta_1-\theta_2) \cdot \mathbf{1}( x_i < \theta_1+\theta_2) ) \\
&= \left( \frac{1}{2 \theta_2} \right)^n \mathbf{1}( { X_{(1)} > \theta_1-\theta_2}) \cdot \mathbf{1}( X_{(n)} < \theta_1 + \theta_2)
\end{align*}
Noting $h(X) = 1$ and $g(\mathbf{\theta}, T(\underline{X})$ is everything else, we see we can split the joint pdf into a piece that depends only on data, and a piece where the random variable only interacts with the parameters through the sufficient statistics, $T(\underline{X}) = (X_{(1)}, X_{(n)})$. 

\textbf{Completeness:} Recall that a statistic is complete iff 
\[ E_\theta(g(T(\underline{X}))) = 0, \;\; \forall \theta\ \Rightarrow\  g(T(\underline{X}) = 0 \text{ a.s.} \]

For ease of notation, let $R = X_{(n)} - X_{(1)}$. The joint pdf of the min and max is given by
\[ \frac{n!}{(n-2)!} \frac{1}{\left( 2 \theta_2 \right)^2} \left( \frac{R}{2 \theta_2} \right) ^{n-2} \]
\begin{align*}
E_\theta(g(T(\underline{X}))) &= \int_{\underline{\theta}} \frac{n \cdot (n-1)}{\left( 2 \theta_2 \right)^n } \cdot R^{n-2} g(X_{(1)}, X_{(n)}) dT(\underline{X}) = 0 \\
& \Rightarrow  \int_{\underline{\theta}} R^{n-2} g(X_{(1)}, X_{(n)}) dT(\underline{X}) = 0 \\
& \Rightarrow  \int_{\underline{\theta}} \left( R^{n-2} g(X_{(1)}, X_{(n)}) \right)^+ dT(\underline{X}) = \int_{\underline{\theta}} \left( R^{n-2} g(X_{(1)}, X_{(n)}) \right)^- dT(\underline{X})
\end{align*}
Since both integrands are positive, this suggests that 
\[ \left( R^{n-2} g(X_{(1)}, X_{(n)}) \right)^+ dT(\underline{X}) = \left( R^{n-2} g(X_{(1)}, X_{(n)}) \right)^-, \]
which will occur only when $R^{n-2} g(X_{(1)}, X_{(n)}) = 0$ almost surely. Since the range is a positive value, this implies that $g(T(\underline{X}) = 0$ almost surely. This proves that the min and max are complete. 
\newline
\newline
\emph{Show that 
\[ E(X_{(1)}) = \theta_1 - \theta_2 + \frac{2 \theta_2}{n+1} \] and 
\[ E(X_{(n)}) = \theta_1 + \theta_2 - \frac{2 \theta_2}{n+1} \] }
It can be shown that
\[ f(X_{(1)}) = \frac{n}{ 2 \theta_2 } \left( 1 - \frac{X_{(1)} - (\theta_1-\theta_2)}{2 \theta_2} \right)^{n-1} \]
\[ f(X_{(n)}) = \frac{n}{\left( 2 \theta_2 \right)^n} \left( X_{(n)} - (\theta_1 - \theta_2) \right)^{n-1} \]

\begin{align*}
E(X_{(1)}) &= \int_{\theta_1 - \theta_2}^{\theta_1 + \theta_2} \frac{nx}{2 \theta_2} \left( 1-\frac{x-(\theta_1 - \theta_2)}{2 \theta_2} \right)^{n-1} dx \\
	&= \left[ -x\left( 1 - \frac{x - (\theta_1-\theta_2)}{2 \theta_2} \right)^n \right]_{\theta_1 -\theta_2}^{\theta_1 + \theta_2} + \int_{\theta_1 - \theta_2}^{\theta_1 + \theta_2} \left( 1-\frac{x-(\theta_1 - \theta_2)}{2 \theta_2} \right)^{n} dx \\
	&= \theta_1 - \theta_2 - \frac{2 \theta_2}{n+1} \left[ \left( 1 - \frac{x - (\theta_1-\theta_2)}{2 \theta_2} \right)^{n+1} \right]_{\theta_1 -\theta_2}^{\theta_1 + \theta_2} \\
	&= \theta_1 - \theta_2 + \frac{2 \theta_2}{n+1} \\
%
E(X_{(n)}) &= \int_{\theta_1 - \theta_2}^{\theta_1 + \theta_2} \frac{nx}{2 \theta_2} \left( \frac{x-(\theta_1 - \theta_2)}{2 \theta_2} \right)^{n-1} dx \\
	&= \left[ x\left( \frac{x - (\theta_1-\theta_2)}{2 \theta_2} \right)^n \right]_{\theta_1 -\theta_2}^{\theta_1 + \theta_2} - \int_{\theta_1 - \theta_2}^{\theta_1 + \theta_2} \left( \frac{x-(\theta_1 - \theta_2)}{2 \theta_2} \right)^{n} dx \\
	&= \theta_1 + \theta_2 - \frac{2 \theta_2}{n+1} \left[ \left( \frac{x - (\theta_1-\theta_2)}{2 \theta_2} \right)^{n+1} \right]_{\theta_1 -\theta_2}^{\theta_1 + \theta_2} \\
	&= \theta_1 + \theta_2 - \frac{2 \theta_2}{n+1}
\end{align*}
\newline
\newline
\emph{Find the UMVUE for $\theta_1$ and $\theta_2$.}

Find, we find unbiased estimators for $\theta_1$ and $\theta_2$ using the sample min and max.
\[ E( X_{(1)} + X_{(n)} ) = \left( \theta_1 - \theta_2 + \frac{2 \theta_2}{n+1} \right) + \left( \theta_1 + \theta_2 - \frac{2 \theta_2}{n+1} \right) = 2 \theta_1 \]
\[ \Rightarrow E \left[ \frac{( X_{(1)} + X_{(n)} )}{2} \right] = \theta_1 \]
\[ E( X_{(n)} - X_{(1)} ) = \left( \theta_1 + \theta_2 - \frac{2 \theta_2}{n+1} \right) - \left( \theta_1 - \theta_2 + \frac{2 \theta_2}{n+1} \right) = 2 \theta_2 - \frac{4 \theta_2}{n+1} \]
\[ \Rightarrow E \left[ \frac{ (n+1) \left( X_{(n)} - X_{(1)} \right) }{2n-2} \right] = \theta_2 \]

Since they depend only on sufficient and complete statistics, the Lehmann-Scheff\'{e} Theorem tells us that the two estimators are UMVUE for $\theta_1, \theta_2$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%